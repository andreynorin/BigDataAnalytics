{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\BigDataAnalytics\\Classifier\\Data\n"
     ]
    }
   ],
   "source": [
    "# set present working directory\n",
    "%cd \"E:\\BigDataAnalytics\\Classifier\\Data\"\n",
    "\n",
    "# set filenames\n",
    "\n",
    "# input JSON file name\n",
    "fileName            = 'bhbkh2jowm4ztp4bckxxuwsuje.json'\n",
    "\n",
    "fileNameNoExt       = fileName.split('.')[0]\n",
    "parquetRawFileName  = fileNameNoExt +'_raw.parkquet'\n",
    "parquetVecFileName  = fileNameNoExt +'_vectorized.parkquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy_sentence_bert\n",
    "\n",
    "# load one of the models listed at https://github.com/MartinoMensio/spacy-sentence-bert/\n",
    "nlp = spacy_sentence_bert.load_model('en_stsb_distilbert_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\BigDataAnalytics\\Classifier\\Data\n"
     ]
    }
   ],
   "source": [
    "%cd \"E:\\BigDataAnalytics\\Classifier\\Data\"\n",
    "df = pd.read_parquet(parquetRawFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split df into chunks\n",
    "def split_dataframe(df, chunk_size = 100): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks\n",
    "\n",
    "vectorizedDfs = split_dataframe(df, chunk_size = 10000)\n",
    "len(vectorizedDfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"E:\\BigDataAnalytics\\Classifier\\Data\\VectorizedData\"\n",
    "\n",
    "for i in range(0,len(vectorizedDfs)+1):\n",
    "#for i in range(0,2):\n",
    "    parquetVecFileName  = f\"{fileNameNoExt}_{i}_vectorized.parkquet\"\n",
    "    print(parquetVecFileName)\n",
    "    df = vectorizedDfs[i]\n",
    "    df['vector'] = df['title'].apply(lambda x: nlp(x).vector)\n",
    "    df.to_parquet(parquetVecFileName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the parquet files into a single pandas df\n",
    "%cd \"E:\\BigDataAnalytics\\Classifier\\Data\\VectorizedData\"\n",
    "full_df = pd.read_parquet('E:\\\\BigDataAnalytics\\\\Classifier\\\\Data\\\\VectorizedData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3df0aa79156feb5fb6258c1c6cd6690224a10fc2ae8a5ac50eb8b2d8a72ebe0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
