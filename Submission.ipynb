{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading CLI commands\n",
    "\n",
    "mkdir GoogleTrainingData\n",
    "cd GoogleTrainingData/\n",
    "aws s3 cp s3://litter-box/GoogleNewsTrainingData/Random20Files .  --recursive\n",
    "hdfs dfs -mkdir hdfs:///GoogleTrainingData\n",
    "hdfs dfs -put * hdfs:///GoogleTrainingData\n",
    "mkdir CC-News-En-Titles-Only\n",
    "aws s3 cp s3://litter-box/CC-News-En-Titles-Only/ ./CC-News-En-Titles-Only --recursive\n",
    "hdfs dfs -mkdir hdfs:///CC-News-En-Titles-Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is used to extract page titles from WARC files and save them into a Parquet format\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import int64\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# set filenames\n",
    "WARCfileName         = 'CC-NEWS-20160828145159-00004_ENG.warc.gz'\n",
    "WARCfileNameNoExt   = WARCfileName.split('.')[0]\n",
    "parquetFileName     = WARCfileName +'.parquet'\n",
    "\n",
    "def build_titles_df():\n",
    "    with open(WARCfileName, 'rb') as stream:\n",
    "            recordCounter = 0\n",
    "            for record in ArchiveIterator(stream):\n",
    "                    if record.rec_type == 'response':\n",
    "                        payload_content = record.raw_stream.read()\n",
    "                        soup             = BeautifulSoup(payload_content, 'html.parser')\n",
    "                        if (soup.title is not None):\n",
    "                            title = soup.title.string\n",
    "                            df.loc[recordCounter] = [title]\n",
    "\n",
    "                    recordCounter += 1\n",
    "\n",
    "    df.head()\n",
    "\n",
    "print(\"Generating dataframe\")\n",
    "df = pd.DataFrame(columns=(['Title']))\n",
    "\n",
    "print(\"Working....\")\n",
    "build_titles_df()\n",
    "\n",
    "df.to_parquet(parquetFileName)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is run in the PySpark prompt\n",
    "\n",
    "parDF1=spark.read.parquet(\"hdfs:///GoogleTrainingData/*.parkquet\")\n",
    "parDF1.count()\n",
    "\n",
    "parDF2=spark.read.parquet(\"hdfs:///CC-News-En-Titles-Only/*.parkquet\")\n",
    "parDF2.count()\n",
    "\n",
    "parDF1 = parDF1.withColumn('index', f.monotonically_increasing_id())\n",
    "\n",
    "trainDataset = parDF1\n",
    "testDataset = parDF2\n",
    "\n",
    "#trainDataset = parDF1.sort('index').limit(16000)\n",
    "#testDataset = parDF1.sort('index', ascending = False).limit(4000)\n",
    "\n",
    "testDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "trainDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() .setInputCol(\"title\") .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() .setInputCols([\"document\"]) .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings().pretrained(name='small_bert_L4_256', lang='en') .setInputCols([\"document\",'token']).setOutputCol(\"embeddings\")\n",
    "\n",
    "embeddingsSentence = SentenceEmbeddings() .setInputCols([\"document\", \"embeddings\"]) .setOutputCol(\"sentence_embeddings\") .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "classsifierdl = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"topic\").setMaxEpochs(10).setLr(0.001).setBatchSize(8).setEnableOutputLogs(True)#.setOutputLogsPath('logs')\n",
    "\n",
    "bert_clf_pipeline = Pipeline(stages=[document_assembler,tokenizer,bert_embeddings,embeddingsSentence,classsifierdl])\n",
    "\n",
    "# training the model - this may take a fairly long time\n",
    "# for 5 files start time - 10:14am, end time -  10:22am\n",
    "bert_clf_pipelineModel = bert_clf_pipeline.fit(trainDataset)\n",
    "\n",
    "# make predictions\n",
    "preds = bert_clf_pipelineModel.transform(testDataset)\n",
    "preds_df = preds.select('topic','title','class.result')\n",
    "preds_df.count()\n",
    "preds_df.show(20)\n",
    "\n",
    "#exporting to parkquet to perform analysis in a regular Jupyter notebook\n",
    "preds_df.write.parquet(\"hdfs:///preds_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is used to evaluate classifier accuracy\n",
    "# it is run in Jupyter notebook because I wasnt able to load scikit-learn (sklearn) on EMR\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_df = pd.read_parquet(\"part-00000-18296d15-3f4f-447f-9ec3-9c4ad3a8d2ed-c000.snappy.parquet\")\n",
    "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
    "print (classification_report(preds_df['topic'], preds_df['result']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3df0aa79156feb5fb6258c1c6cd6690224a10fc2ae8a5ac50eb8b2d8a72ebe0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
