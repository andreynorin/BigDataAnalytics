{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on any EC2 or locally\n",
    "# warcio recompress command may need to be run on each WARC dataset file as sometimes files dont't parse\n",
    "# the following code is used to extract page titles from WARC files and save them into a Parquet format\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import int64\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# set filenames\n",
    "WARCfileName         = 'CC-NEWS-20160828145159-00004_ENG.warc.gz'\n",
    "WARCfileNameNoExt   = WARCfileName.split('.')[0]\n",
    "parquetFileName     = WARCfileName +'.parquet'\n",
    "\n",
    "def build_titles_df():\n",
    "    with open(WARCfileName, 'rb') as stream:\n",
    "            recordCounter = 0\n",
    "            for record in ArchiveIterator(stream):\n",
    "                    if record.rec_type == 'response':\n",
    "                        payload_content = record.raw_stream.read()\n",
    "                        soup             = BeautifulSoup(payload_content, 'html.parser')\n",
    "                        if (soup.title is not None):\n",
    "                            title = soup.title.string\n",
    "                            df.loc[recordCounter] = [title]\n",
    "\n",
    "                    recordCounter += 1\n",
    "\n",
    "    df.head()\n",
    "\n",
    "print(\"Generating dataframe\")\n",
    "df = pd.DataFrame(columns=(['Title']))\n",
    "\n",
    "print(\"Working....\")\n",
    "build_titles_df()\n",
    "\n",
    "df.to_parquet(parquetFileName)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading CLI commands\n",
    "\n",
    "mkdir GoogleTrainingData\n",
    "cd GoogleTrainingData/\n",
    "aws s3 cp s3://litter-box/GoogleNewsTrainingData/Random20Files .  --recursive\n",
    "hdfs dfs -mkdir hdfs:///GoogleTrainingData\n",
    "hdfs dfs -put * hdfs:///GoogleTrainingData\n",
    "mkdir CC-News-En-Titles-Only\n",
    "aws s3 cp s3://litter-box/CC-News-En-Titles-Only/ ./CC-News-En-Titles-Only --recursive\n",
    "hdfs dfs -mkdir hdfs:///CC-News-En-Titles-Only\n",
    "hdfs dfs -put * hdfs:///CC-News-En-Titles-Only\n",
    "hdfs dfs -mkdir hdfs:///PredictionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is run in the PySpark prompt\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "parDF1=spark.read.parquet(\"hdfs:///GoogleTrainingData/*.parkquet\")\n",
    "parDF1.count()\n",
    "\n",
    "parDF2=spark.read.parquet(\"hdfs:///CC-News-En-Titles-Only/*.parquet\")\n",
    "parDF2.count()\n",
    "\n",
    "parDF1 = parDF1.withColumn('index', f.monotonically_increasing_id())\n",
    "\n",
    "trainDataset    = parDF1\n",
    "predictDataset  = parDF2\n",
    "\n",
    "# splitting the dataset int 160k and 40k records for train/test\n",
    "trainDataset = parDF1.sort('index').limit(160000)\n",
    "testDataset = parDF1.sort('index', ascending = False).limit(40000)\n",
    "\n",
    "trainDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "testDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() .setInputCol(\"title\") .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() .setInputCols([\"document\"]) .setOutputCol(\"token\")\n",
    "\n",
    "bert_embeddings = BertEmbeddings().pretrained(name='small_bert_L4_256', lang='en') .setInputCols([\"document\",'token']).setOutputCol(\"embeddings\")\n",
    "\n",
    "embeddingsSentence = SentenceEmbeddings() .setInputCols([\"document\", \"embeddings\"]) .setOutputCol(\"sentence_embeddings\") .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "classsifierdl = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"topic\").setMaxEpochs(10).setLr(0.001).setBatchSize(8).setEnableOutputLogs(True)#.setOutputLogsPath('logs')\n",
    "\n",
    "bert_clf_pipeline = Pipeline(stages=[document_assembler,tokenizer,bert_embeddings,embeddingsSentence,classsifierdl])\n",
    "\n",
    "# training the model - this may take a fairly long time\n",
    "# for 5 files start time - 10:14am, end time -  10:22am\n",
    "bert_clf_pipelineModel = bert_clf_pipeline.fit(trainDataset)\n",
    "\n",
    "# make sanity check predictions\n",
    "preds = bert_clf_pipelineModel.transform(testDataset)\n",
    "preds_df = preds.select('topic','title','class.result')\n",
    "preds_df.count()\n",
    "preds_df.show(20)\n",
    "\n",
    "# make predictions against CC-News-En dataset\n",
    "preds = bert_clf_pipelineModel.transform(predictDataset)\n",
    "preds_df = preds.select('topic','title','class.result')\n",
    "preds_df.count()\n",
    "preds_df.show(20)\n",
    "\n",
    "#exporting to parkquet to perform analysis in a regular Jupyter notebook\n",
    "preds_df.write.parquet(\"hdfs:///preds_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANDRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ANDRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BUSINESS       0.67      0.70      0.68      3642\n",
      "ENTERTAINMENT       0.69      0.76      0.73      2862\n",
      "       HEALTH       0.51      0.76      0.61      1739\n",
      "       NATION       0.52      0.67      0.59      3540\n",
      "      SCIENCE       0.00      0.00      0.00       439\n",
      "       SPORTS       0.79      0.83      0.81      3755\n",
      "   TECHNOLOGY       0.70      0.67      0.69      2026\n",
      "        WORLD       0.00      0.00      0.00      1997\n",
      "\n",
      "     accuracy                           0.65     20000\n",
      "    macro avg       0.49      0.55      0.51     20000\n",
      " weighted avg       0.58      0.65      0.61     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANDRE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# the following code is used to evaluate classifier accuracy\n",
    "# it is run in Jupyter notebook because I wasnt able to load scikit-learn (sklearn) on EMR\n",
    "# accuracy matrix is just a sanity check example, full model is still learning/processing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds_df = pd.read_parquet(\"part-00000-18296d15-3f4f-447f-9ec3-9c4ad3a8d2ed-c000.snappy.parquet\")\n",
    "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
    "print (classification_report(preds_df['topic'], preds_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the test/train dataset and the number labelled examples for training\n",
    "\n",
    ">>> trainDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "+-------------+-----+\n",
    "|        topic|count|\n",
    "+-------------+-----+\n",
    "|       SPORTS|29781|\n",
    "|     BUSINESS|28577|\n",
    "|       NATION|28416|\n",
    "|ENTERTAINMENT|23518|\n",
    "|        WORLD|16472|\n",
    "|   TECHNOLOGY|15406|\n",
    "|       HEALTH|14295|\n",
    "|      SCIENCE| 3535|\n",
    "+-------------+-----+\n",
    "\n",
    ">>> testDataset.groupBy(\"topic\").count().orderBy(col(\"count\").desc()).show()\n",
    "+-------------+-----+\n",
    "|        topic|count|\n",
    "+-------------+-----+\n",
    "|       SPORTS| 7525|\n",
    "|     BUSINESS| 7206|\n",
    "|       NATION| 7038|\n",
    "|ENTERTAINMENT| 5737|\n",
    "|        WORLD| 4157|\n",
    "|   TECHNOLOGY| 3923|\n",
    "|       HEALTH| 3587|\n",
    "|      SCIENCE|  827|\n",
    "+-------------+-----+\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3df0aa79156feb5fb6258c1c6cd6690224a10fc2ae8a5ac50eb8b2d8a72ebe0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
